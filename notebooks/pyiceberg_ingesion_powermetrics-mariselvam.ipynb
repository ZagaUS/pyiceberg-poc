{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "try:\n",
    "        # Create KafkaConsumer\n",
    "        consumer = KafkaConsumer(\n",
    "            \"powerlevel\",\n",
    "            bootstrap_servers=['telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443'],\n",
    "            value_deserializer=lambda v: v,  # Keep value as bytes for JSON decoding\n",
    "            group_id=\"sustaingroup01\",\n",
    "            auto_offset_reset='earliest',\n",
    "            consumer_timeout_ms=20000,\n",
    "            security_protocol='SSL',  # Add SSL configuration if needed\n",
    "            ssl_cafile='./telemetry_zagaopenshift.pem',\n",
    "            ssl_certfile='./telemetry_zagaopenshift.crt'\n",
    "        )\n",
    "        print(\"Kafka consumer created successfully.\")\n",
    "except Exception as e:\n",
    "        print(f\"Error creating Kafka consumer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "try:\n",
    "        # logger.info(\"Starting message consumption\")\n",
    "    for message in consumer:\n",
    "            # logger.debug(f\"Consumed message: {message.value}\")\n",
    "            json_message = json.loads(message.value.decode('utf-8'))  # Decode and parse JSON\n",
    "            messages.append(json_message)\n",
    "            if len(messages) > 0:\n",
    "                print(\"Message consumed\")\n",
    "                consumer.close()\n",
    "except KeyboardInterrupt:\n",
    "        # logger.info(\"Stopping consumer...\")\n",
    "        print(\"Keyboard Interrupt\")\n",
    "except Exception as e:\n",
    "        # logger.error(f\"An error occurred: {e}\")\n",
    "        print(e)\n",
    "finally:\n",
    "        print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "catalog = load_catalog(\n",
    "        \"docs\",\n",
    "    **{\n",
    "            \"uri\": \"thrift://hive-ms-trino.apps.zagaopenshift.zagaopensource.com:9888\",\n",
    "            \"s3.endpoint\": \"http://minio-lb.apps.zagaopenshift.zagaopensource.com:9009\",\n",
    "            \"py-io-impl\": \"pyiceberg.io.pyarrow.PyArrowFileIO\",\n",
    "            \"s3.access-key-id\": \"minioAdmin\",\n",
    "            \"s3.secret-access-key\": \"minio1234\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'docs_powerleveldata'  # Replace with your namespace (database) name\n",
    "tables = catalog.list_tables(namespace)\n",
    "\n",
    "# Print the list of tables\n",
    "for table in tables:\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"docs_powerleveldata.demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new data to the table\n",
    "with table.overwrite() as writer:\n",
    "    for record in messages:\n",
    "        writer(record)\n",
    "print(table.scan().to_arrow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_arrow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.create_namespace(\"docs_powerleveldata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list_namespaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    StructType,\n",
    "    ListType\n",
    ")\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import DayTransform\n",
    "from pyiceberg.table.sorting import SortOrder, SortField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "\n",
    "# Define the schema for the spans\n",
    "schema = Schema(\n",
    "    NestedField(\n",
    "        field_id=1,\n",
    "        name=\"alldata\",  # Name of the column to store all data\n",
    "        field_type=StringType(),\n",
    "        required=False\n",
    "    )\n",
    ")\n",
    "\n",
    "table = catalog.create_table(\n",
    "        identifier=\"docs_powerleveldata.demo2\", #mendtion identifier for namespace and create a table name\n",
    "        schema=schema ,\n",
    "        location=\"s3a://mariselvam/powerleveldata2\" ,#mention the  minio bucket name and mendtion path name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.io.pyarrow import PyArrowFileIO\n",
    "import pyarrow as pa\n",
    "import json\n",
    "\n",
    "# Serialize each message into a JSON string\n",
    "serialized_data = [json.dumps(msg) for msg in messages]\n",
    "\n",
    "# Create a PyArrow Table with a single column containing the serialized data\n",
    "df = pa.Table.from_pylist([{\"alldata\": data} for data in serialized_data])\n",
    "table.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_arrow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import StringType, NestedField\n",
    "from pyiceberg.io.pyarrow import PyArrowFileIO\n",
    "import pyarrow as pa\n",
    "\n",
    "def create_kafka_consumer():\n",
    "    try:\n",
    "        consumer = KafkaConsumer(\n",
    "            \"powerlevel\",\n",
    "            bootstrap_servers=['telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443'],\n",
    "            value_deserializer=lambda v: v,  # Keep value as bytes for JSON decoding\n",
    "            group_id=\"sustaingroup01\",\n",
    "            auto_offset_reset='earliest',\n",
    "            consumer_timeout_ms=20000,\n",
    "            security_protocol='SSL',  # Add SSL configuration if needed\n",
    "            ssl_cafile='./telemetry_zagaopenshift.pem',\n",
    "            ssl_certfile='./telemetry_zagaopenshift.crt'\n",
    "        )\n",
    "        print(\"Kafka consumer created successfully.\")\n",
    "        return consumer\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Kafka consumer: {e}\")\n",
    "        return None\n",
    "\n",
    "def consume_messages(consumer):\n",
    "    messages = []\n",
    "    try:\n",
    "        for message in consumer:\n",
    "            json_message = json.loads(message.value.decode('utf-8'))  # Decode and parse JSON\n",
    "            messages.append(json_message)\n",
    "            if len(messages) > 0:\n",
    "                print(\"Message consumed\")\n",
    "                consumer.close()\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard Interrupt\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        print(messages)\n",
    "    return messages\n",
    "\n",
    "def load_catalog_and_list_tables():\n",
    "    catalog = load_catalog(\n",
    "        \"docs\",\n",
    "        **{\n",
    "            \"uri\": \"thrift://hive-ms-trino.apps.zagaopenshift.zagaopensource.com:9888\",\n",
    "            \"s3.endpoint\": \"http://minio-lb.apps.zagaopenshift.zagaopensource.com:9009\",\n",
    "            \"py-io-impl\": \"pyiceberg.io.pyarrow.PyArrowFileIO\",\n",
    "            \"s3.access-key-id\": \"minioAdmin\",\n",
    "            \"s3.secret-access-key\": \"minio1234\",\n",
    "        }\n",
    "    )\n",
    "    namespace = 'docs_powerleveldata'  # Replace with your namespace (database) name\n",
    "    tables = catalog.list_tables(namespace)\n",
    "    for table in tables:\n",
    "        print(table)\n",
    "    return catalog, tables\n",
    "\n",
    "def create_or_load_table(catalog, tables):\n",
    "    table_name = \"demo5\"\n",
    "    full_table_name = ('docs_powerleveldata', table_name)\n",
    "\n",
    "    if full_table_name in tables:\n",
    "        print(f\"Table {full_table_name} already exists.\")\n",
    "        table = catalog.load_table(full_table_name)\n",
    "    else:\n",
    "        print(f\"Table {full_table_name} does not exist. Creating table...\")\n",
    "        schema = Schema(\n",
    "            NestedField(\n",
    "                field_id=1,\n",
    "                name=\"alldata\",  # Name of the column to store all data\n",
    "                field_type=StringType(),\n",
    "                required=False\n",
    "            )\n",
    "        )\n",
    "        table = catalog.create_table(\n",
    "            identifier=full_table_name,  # Mention identifier for namespace and create a table name\n",
    "            schema=schema,\n",
    "            location=\"s3a://mariselvam/powerlevelsampledata\"  # Mention the Minio bucket name and mention path name\n",
    "        )\n",
    "    return table\n",
    "\n",
    "def append_data_to_table(table, messages):\n",
    "    # Serialize each message into a JSON string\n",
    "    serialized_data = [json.dumps(msg) for msg in messages]\n",
    "\n",
    "# Create a PyArrow Table with a single column containing the serialized data\n",
    "    df = pa.Table.from_pylist([{\"alldata\": data} for data in serialized_data])\n",
    "    table.append(df)\n",
    "    \n",
    "    # Append data to the Iceberg table\n",
    "    # with table.new_append() as appender:\n",
    "    #     appender.append_table(df)\n",
    "    #     appender.commit()\n",
    "\n",
    "def main():\n",
    "    consumer = create_kafka_consumer()\n",
    "    if not consumer:\n",
    "        return\n",
    "    \n",
    "    messages = consume_messages(consumer)\n",
    "    if not messages:\n",
    "        return\n",
    "    \n",
    "    catalog, tables = load_catalog_and_list_tables()\n",
    "    table = create_or_load_table(catalog, tables)\n",
    "    append_data_to_table(table, messages)\n",
    "    print(\"Data appended successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
