{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyiceberg pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyiceberg.catalog import load_catalog\n",
    "# from pyiceberg.schema import Schema\n",
    "# from pyiceberg.types import StringType, IntegerType, NestedField\n",
    "# from pyiceberg.partitioning import PartitionSpec\n",
    "from kafka import KafkaConsumer\n",
    "from kafka.errors import NoBrokersAvailable\n",
    "import json\n",
    "import logging\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    StructType,\n",
    ")\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import DayTransform\n",
    "from pyiceberg.table.sorting import SortOrder, SortField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "# Configure logging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger(__name__)  # Corrected __name__\n",
    "logger = logging.getLogger(__name__)\n",
    "# Set the logging level to INFO to suppress DEBUG messages\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# # Explicitly set the logging level for Kafka loggers\n",
    "# logging.getLogger('kafka').setLevel(logging.WARNING)\n",
    "# logging.getLogger('kafka.coordinator').setLevel(logging.WARNING)\n",
    "# logging.getLogger('kafka.protocol.parser').setLevel(logging.WARNING)\n",
    "# logging.getLogger('kafka.conn').setLevel(logging.WARNING)\n",
    "# logging.getLogger('kafka.client').setLevel(logging.WARNING)\n",
    "# logging.getLogger('kafka.metrics.metrics').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'powerlevel'\n",
    "bootstrap_servers = ['telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443']\n",
    "group_id = 'sustaingroup01'\n",
    "# SSL Configuration\n",
    "ssl_certfile = './telemetry_zagaopenshift.crt'\n",
    "ssl_cafile = './telemetry_zagaopenshift.pem'\n",
    "# security_protocol = 'SSL'\n",
    "  # Replace with your consumer group ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:kafka.metrics.metrics:Added sensor with name connections-closed\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name connections-created\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name select-time\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name io-time\n",
      "DEBUG:kafka.client:Initiating connection to node bootstrap-0 at telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name bytes-sent-received\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name bytes-sent\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name bytes-received\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name request-latency\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name node-bootstrap-0.bytes-sent\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name node-bootstrap-0.bytes-received\n",
      "DEBUG:kafka.metrics.metrics:Added sensor with name node-bootstrap-0.latency\n",
      "DEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443 <disconnected> [unspecified None]>: creating new socket\n",
      "DEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443 <disconnected> [IPv4 ('120.56.39.13', 443)]>: setting socket option (6, 1, 1)\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443 <connecting> [IPv4 ('120.56.39.13', 443)]>: connecting to telemetry-kafka-external-bootstrap-observability-kafka.apps.zagaopenshift.zagaopensource.com:443 [('120.56.39.13', 443) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "ERROR:__main__:No brokers available. Ensure the Kafka broker is running and accessible. Error: NoBrokersAvailable\n"
     ]
    }
   ],
   "source": [
    "consumer = None\n",
    "try:\n",
    "        # Create KafkaConsumer\n",
    "        consumer = KafkaConsumer(\n",
    "            topic,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            value_deserializer=lambda v: v,  # Keep value as bytes for JSON decoding\n",
    "            group_id=group_id,\n",
    "            auto_offset_reset='earliest',\n",
    "            consumer_timeout_ms=20000,\n",
    "            security_protocol='SSL',  # Add SSL configuration if needed\n",
    "            ssl_cafile=ssl_cafile,\n",
    "            ssl_certfile=ssl_certfile\n",
    "        )\n",
    "        logger.info(\"Successfully connected to Kafka broker\")\n",
    "except NoBrokersAvailable as e:\n",
    "    logger.error(f\"No brokers available. Ensure the Kafka broker is running and accessible. Error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create Kafka consumer. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_messages(consumer):\n",
    "    messages = []\n",
    "    try:\n",
    "        logger.info(\"Starting message consumption\")\n",
    "        for message in consumer:\n",
    "            logger.debug(f\"Consumed message: {message.value}\")\n",
    "            json_message = json.loads(message.value.decode('utf-8'))  # Decode and parse JSON\n",
    "            messages.append(json_message)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Stopping consumer...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # consumer.close()\n",
    "        logger.info(\"Consumer closed\")\n",
    "    return messages\n",
    "consume_messages(consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_iceberg_table():\n",
    "    # S3 bucket name\n",
    "    s3_bucket_name = \"powerlevel-pod\"\n",
    "    catalog = load_catalog(\n",
    "        \"hive\",\n",
    "    **{\n",
    "        \"uri\": \"thrift://192.168.31.102:9083\", # For other networks use this URL: \"hive-ms-trino.apps.zagaopenshift.zagaopensource.com 9888\"\n",
    "        \"s3.endpoint\": \"http://minio-api-dev.apps.zagaopenshift.zagaopensource.com\",\n",
    "        \"s3.access-key-id\": \"minio\",\n",
    "        \"s3.secret-access-key\": \"minio123$\",\n",
    "    }\n",
    "    )\n",
    "    catalog.create_namespace(\"power_pod\")\n",
    "    \n",
    "    schema = Schema(\n",
    "        NestedField(field_id=1, name=\"resourceMetrics\", field_type=StringType(), required=True),        \n",
    "        NestedField(field_id=2, name=\"container_id\", field_type=StringType(), required=True),\n",
    "        NestedField(field_id=3, name=\"container_name\", field_type=StringType(), required=True),\n",
    "        NestedField(field_id=4, name=\"container_namespace\", field_type=StringType(), required=True),\n",
    "        NestedField(field_id=5, name=\"pod_name\", field_type=StringType(), required=False),\n",
    "        NestedField(field_id=6, name=\"timeUnixNano\", field_type=LongType(), required=False),\n",
    "        NestedField(field_id=7, name=\"asDouble\", field_type=StringType(), required=False),        \n",
    "    )\n",
    "\n",
    "    partition_spec = PartitionSpec(\n",
    "            PartitionField(\n",
    "                source_id=2, field_id=1000, transform=DayTransform(), name=\"container_id\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sort_order = SortOrder(SortField(source_id=1, transform=IdentityTransform()))\n",
    "\n",
    "    table = catalog.create_table(\n",
    "            identifier=\"power_pod\",\n",
    "            schema=schema,\n",
    "            location=f\"s3a://{s3_bucket_name}/sample\",  # Specify the S3 bucket name here\n",
    "            partition_spec=partition_spec,\n",
    "            sort_order=sort_order,\n",
    "        )\n",
    "        \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_iceberg(table, messages):\n",
    "    import pyarrow as pa\n",
    "    from pyiceberg.io.pyarrow import PyArrowFileIO\n",
    "\n",
    "    # Convert messages to PyArrow Table\n",
    "    data = {\n",
    "        \"resourceMetrics\": [json.dumps(msg) for msg in messages],  # Store the entire message as a string\n",
    "        \"container_id\": [msg.get(\"container_id\") for msg in messages],\n",
    "        \"container_name\": [msg.get(\"container_name\") for msg in messages],\n",
    "        \"container_namespace\": [msg.get(\"container_namespace\") for msg in messages],\n",
    "        \"pod_name\": [msg.get(\"pod_name\") for msg in messages],\n",
    "        \"timeUnixNano\": [msg.get(\"timeUnixNano\") for msg in messages],\n",
    "        \"asDouble\": [msg.get(\"asDouble\") for msg in messages],\n",
    "    }\n",
    "\n",
    "    arrow_table = pa.Table.from_pydict(data)\n",
    "    \n",
    "    # Write the data to the Iceberg table\n",
    "    file_io = PyArrowFileIO()\n",
    "    with table.new_append(file_io) as writer:\n",
    "        writer.add(arrow_table)\n",
    "        writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if the consumer is correctly created and not None\n",
    "    if consumer is not None:\n",
    "        try:\n",
    "            # Consume messages\n",
    "            for json_response in consume_messages(consumer):\n",
    "                print(\"success\")\n",
    "                print(json.dumps(json_response, indent=2))  # Pretty-print JSON response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error consuming messages: {e}\")\n",
    "        finally:\n",
    "            consumer.close()\n",
    "            logger.info(\"Consumer closed\")\n",
    "    else:\n",
    "        logger.error(\"Consumer initialization failed. Exiting...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
